{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8c5a07",
   "metadata": {},
   "source": [
    "# 2nd Extension\n",
    "\n",
    "In this 2nd Extension to the Specter Paper, we always considered the fully pre-trained Specter Embedder and used this term interchangeably with Specter.\n",
    "\n",
    "The [Specter Paper](https://arxiv.org/abs/2004.07180) suggests that classification of the embeddings $e_i \\in \\mathbb R^{d}\\ \\forall i \\in \\mathcal X$ can be carried out using standard classical Machine Learning Algorithms such as SVM. \n",
    "\n",
    "In this case, a discriminative function $\\hat f$ is learned and applied on the embeddings $e_i$ to perform classification. \n",
    "\n",
    "The intuition behind this idea (leading to the 1-digit $\\Delta$-improvement in performance the authors of Specter did achieve) is that one can decouple the task of Text Classification into two main subcomponents: \n",
    "\n",
    "1. **Natural Language Embedding**, i.e. to obtain contextualized numerical representations of textual data\n",
    "\n",
    "2. **Embedding Classification**, i.e. to actually classify these numerical represents using standard Machine Learning techniques.\n",
    "\n",
    "Consider now the following Figure, representing two alternative systems for Specter-based Text Classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab2554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T19:18:11.945690Z",
     "start_time": "2023-01-02T19:18:11.931924Z"
    }
   },
   "source": [
    "<img src=\"https://i.ibb.co/wJ5zYzp/ext2-scheme.png\" alt=\"ext2-scheme\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e974523",
   "metadata": {},
   "source": [
    "In this picture, two alternative ways with which to classify a given paper $P_i$ are displayed. The process displayed in the top-part of the image is one in which paper $P_i$ is first embedded through $\\texttt{specter}$ into the corresponding embedding $e_i$. Later on, traditional Machine Learning techniques (here represented with the scikit-learn symbol) are used to learn the discriminative function $\\hat f$ (hopefully) minimizing the classification error $\\Vert l - \\hat f(e) \n",
    "\\Vert_{p} \\ \\forall i$ and for some $p$ norm. \n",
    "\n",
    "\n",
    "This process is pretty straight-forward, as the task of Text Classification is here split into the two consituents sub-tasks of NL-Embedding and downstream classification. Clearly enough, the major use of the actual **labelled** data is done in learning $\\hat f$. \n",
    "\n",
    "The bottom diagram shows instead **our** extension, based on the simple yet possibly very powerful intuition that embeddings produced by SPECTER might suffer from over-generalization when used in Text Classification. \n",
    "\n",
    "This directly follows from the fact that said embeddings are produced in the sake of producing high-quality (citation-network informed) embeddings to then later on performs tasks such as Text Classification, but also Citation Prediciton and many more. This clearly hinders the possibility of using SPECTER to its fullest in its applications to Text Classification, since the embeddings it produces might be simply non tailored to be used to this aim. \n",
    "\n",
    "Our intuition is that one can **chain** the two steps on which Text Classification is based unifying the whole process. After an often very extensive and data-intensive phase of pre-training, the embeddings produced by SPECTER are then fed in a Classification Head (CH) based on a Multi-Layer Perceptron. This allows, at least in principle, a complete flow of information between not only the CH parameters and the classification output, but also between the SPECTER embedding model and the classification output itself.\n",
    "\n",
    "Theoretically, this flow of information can be used to tweak (or better, **fine-tune**) SPECTER parameters specifically for classification (or any downstream task really).\n",
    "\n",
    "This is justified by the fact that having a **labelled** dataset $\\tau$ defined as: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\tau =  \\{ \\mathcal P_i \\vert l_i \\}_{i = 1, \\dots, \\vert \\mathcal X \\vert}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then, in the bottom part of the diagram, it is clear that the classification function $g: \\mathcal X \\mapsto \\mathcal L$ is applied to any given paper $\\mathcal P$ as follows: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "g_{\\text{bottom}}(\\mathcal P) = \\bar f(\\texttt{specter}(P))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which yields that if one uses as loss-function the misclassification error $L(l, \\bar l) \\mapsto \\mathbb R^+$ then, clearly enough, one practically observes that:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_\\texttt{specter}} \\neq 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, of course one cannot expect the extensively trained weights of Specter to significantly change for one specific task: as an encoder, Specter's job is, at the end of the day, to turn text into meaningful and dynamic numerical representation. Indeed, the major use of the information in $\\tau$ is used in training the CH on top of Specter. \n",
    "\n",
    "Nevertheless, the embeddings are indeed updated, so that Classification is applied in a dynamical feature space, whose geometry is affected to from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccba4e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T19:18:45.085421Z",
     "start_time": "2023-01-02T19:18:35.152604Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescocapuano/opt/anaconda3/envs/polienv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load specter pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dec980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Extension2.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "01994fa0d2ab1307d682b921dfe369a1a67026eda944b77214d19ac2ebb6a2ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
